import os
import time
from typing import List
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from app.core.config import settings

class RAGService:
    def __init__(self):
        # Kh·ªüi t·∫°o AI v√† Embeddings
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=settings.GOOGLE_API_KEY
        )
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            google_api_key=settings.GOOGLE_API_KEY, 
            temperature=0.3
        )
        # Kh·ªüi t·∫°o Vector Store v·ªõi t√≠nh nƒÉng l∆∞u tr·ªØ vƒ©nh vi·ªÖn
        self.vector_store = Chroma(
            persist_directory=settings.DATABASE_DIR,
            embedding_function=self.embeddings
        )

    def ingest_documents(self, directory_path: str):
        """Nhi·ªám v·ª• Tu·∫ßn 2 & 3: X·ª≠ l√Ω PDF v√† ƒê·∫©y v√†o Vector DB (ƒê√£ fix l·ªói 429)"""
        documents = []
        if not os.path.exists(directory_path):
            print(f"‚ùå L·ªói: Th∆∞ m·ª•c {directory_path} kh√¥ng t·ªìn t·∫°i.")
            return

        # 1. N·∫°p file PDF
        for filename in os.listdir(directory_path):
            if filename.endswith(".pdf"):
                file_path = os.path.join(directory_path, filename)
                try:
                    loader = PyPDFLoader(file_path)
                    docs = loader.load()
                    documents.extend(docs)
                    print(f"‚úÖ ƒê√£ ƒë·ªçc: {filename}")
                except Exception as e:
                    print(f"‚ùå L·ªói file {filename}: {e}")

        if not documents:
            print("‚ùå Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o.")
            return

        # 2. Chunking khoa h·ªçc
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, 
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        splits = text_splitter.split_documents(documents)
        total_chunks = len(splits)
        print(f"üì¶ T·ªïng c·ªông: {total_chunks} ƒëo·∫°n vƒÉn b·∫£n.")

# 3. N·∫°p v√†o Vector DB theo ch·∫ø ƒë·ªô "An To√†n Tuy·ªát ƒê·ªëi"
        batch_size = 1 
        print(f"üöÄ ƒêang n·∫°p t·ª´ng b∆∞·ªõc (C·ª±c ch·∫≠m) ƒë·ªÉ tr√°nh b·ªã ch·∫∑n...")
        
        for i in range(0, total_chunks, batch_size):
            batch = splits[i:i + batch_size]
            try:
                self.vector_store.add_documents(documents=batch)
                print(f"   ‚û§ ƒê√£ n·∫°p th√†nh c√¥ng: {i + 1}/{total_chunks}")
                time.sleep(10)  # Ngh·ªâ 10 gi√¢y m·ªói ƒëo·∫°n
            except Exception as e:
                print(f"‚ö†Ô∏è ƒêang ƒë·ª£i 60 gi√¢y do Google qu√° t·∫£i: {e}")
                time.sleep(60)
                self.vector_store.add_documents(documents=batch)

    def ask_question(self, question: str) -> str:
        """Nhi·ªám v·ª• Tu·∫ßn 4: Truy v·∫•n th√¥ng minh"""
        retriever = self.vector_store.as_retriever(search_kwargs={"k": 5})
        
        system_prompt = (
            "B·∫°n l√† tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ sinh vi√™n d·ª±a tr√™n t√†i li·ªáu n·ªôi b·ªô c·ªßa nh√† tr∆∞·ªùng. "
            "Ch·ªâ s·ª≠ d·ª•ng c√°c ƒëo·∫°n vƒÉn b·∫£n d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. "
            "N·∫øu th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu, h√£y n√≥i 'T√¥i kh√¥ng bi·∫øt'. "
            "C√¢u tr·∫£ l·ªùi c·∫ßn ng·∫Øn g·ªçn, ch√≠nh x√°c v√† l·ªãch s·ª±."
            "\n\n"
            "{context}"
        )

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"),
        ])

        question_answer_chain = create_stuff_documents_chain(self.llm, prompt)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        response = rag_chain.invoke({"input": question})
        return response["answer"]

rag_service = RAGService()