import os
import time
import hashlib
import json
from typing import List
from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from app.core.config import settings
from app.service.embedding_service import ChunkingPresets

class RAGService:
    def __init__(self):
        # Kh·ªüi t·∫°o AI v√† Embeddings
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=settings.GOOGLE_API_KEY
        )
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            google_api_key=settings.GOOGLE_API_KEY, 
            temperature=0.3
        )
        # Kh·ªüi t·∫°o Vector Store v·ªõi t√≠nh nƒÉng l∆∞u tr·ªØ vƒ©nh vi·ªÖn
        self.vector_store = Chroma(
            persist_directory=settings.DATABASE_DIR,
            embedding_function=self.embeddings
        )
        
        # Kh·ªüi t·∫°o ChunkingService (t√°ch b·∫°ch tr√°ch nhi·ªám)
        # C√≥ th·ªÉ s·ª≠ d·ª•ng preset: ChunkingPresets.vietnamese_optimized() (m·∫∑c ƒë·ªãnh)
        self.chunking_service = ChunkingPresets.vietnamese_optimized()
        
        # File ghi nh·∫≠t k√Ω nh·ªØng t√†i li·ªáu ƒë√£ n·∫°p
        self.ingestion_log_file = "./database/ingestion_log.json"
        self._ensure_log_file_exists()
    
    def _ensure_log_file_exists(self):
        """ƒê·∫£m b·∫£o file log t·ªìn t·∫°i"""
        log_dir = os.path.dirname(self.ingestion_log_file)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)
        if not os.path.exists(self.ingestion_log_file):
            with open(self.ingestion_log_file, 'w', encoding='utf-8') as f:
                json.dump({}, f)
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """T√≠nh hash c·ªßa file ƒë·ªÉ ki·ªÉm tra tr√πng l·∫∑p"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    def _load_ingestion_log(self) -> dict:
        """T·∫£i danh s√°ch nh·ªØng file ƒë√£ n·∫°p"""
        try:
            with open(self.ingestion_log_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            return {}
    
    def _save_ingestion_log(self, log_data: dict):
        """L∆∞u danh s√°ch nh·ªØng file ƒë√£ n·∫°p"""
        with open(self.ingestion_log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
    
    def _is_document_already_ingested(self, file_path: str) -> bool:
        """Ki·ªÉm tra xem document ƒë√£ ƒë∆∞·ª£c n·∫°p ch∆∞a"""
        file_hash = self._calculate_file_hash(file_path)
        log_data = self._load_ingestion_log()
        return file_hash in log_data
    
    def _mark_document_as_ingested(self, file_path: str):
        """ƒê√°nh d·∫•u document ƒë√£ ƒë∆∞·ª£c n·∫°p"""
        file_hash = self._calculate_file_hash(file_path)
        log_data = self._load_ingestion_log()
        
        filename = os.path.basename(file_path)
        log_data[file_hash] = {
            'filename': filename,
            'file_path': file_path,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'chunk_size': self.chunking_service.chunk_size,
            'chunk_overlap': self.chunking_service.chunk_overlap,
            'chunk_overlap_percent': f"{self.chunking_service.chunk_overlap_percent*100:.0f}%"
        }
        
        self._save_ingestion_log(log_data)

    def ingest_documents(self, directory_path: str):
        """
        N·∫°p PDF v√†o Vector Database v·ªõi x·ª≠ l√Ω tr√πng l·∫∑p
        
        C·∫•u h√¨nh t·ªëi ∆∞u (t·ª´ test results):
        - chunk_size: 1000 k√Ω t·ª± (m·ª©c trung b√¨nh t·ªëi ∆∞u)
        - chunk_overlap: 180 k√Ω t·ª± (18% - gi·ªØ ng·ªØ c·∫£nh ƒêi·ªÅu/Kho·∫£n)
        - separators: ["\n\n", "\n", ". ", " ", ""] (∆∞u ti√™n ƒëo·∫°n ‚Üí d√≤ng ‚Üí c√¢u ‚Üí t·ª´)
        """
        documents = []
        skipped_files = 0
        
        if not os.path.exists(directory_path):
            print(f"‚ùå L·ªói: Th∆∞ m·ª•c {directory_path} kh√¥ng t·ªìn t·∫°i.")
            return

        print("üì• Ki·ªÉm tra t√†i li·ªáu tr√πng l·∫∑p...")
        
        # 1. N·∫°p file PDF (v·ªõi ki·ªÉm tra tr√πng l·∫∑p)
        for filename in os.listdir(directory_path):
            if filename.endswith(".pdf"):
                file_path = os.path.join(directory_path, filename)
                
                # Ki·ªÉm tra xem document ƒë√£ ƒë∆∞·ª£c n·∫°p ch∆∞a
                if self._is_document_already_ingested(file_path):
                    print(f"‚è≠Ô∏è  B·ªè qua: {filename} (ƒë√£ n·∫°p r·ªìi)")
                    skipped_files += 1
                    continue
                
                try:
                    loader = PyPDFLoader(file_path)
                    docs = loader.load()
                    
                    # C·∫≠p nh·∫≠t metadata
                    for doc in docs:
                        doc.metadata["source"] = filename
                    
                    documents.extend(docs)
                    
                    # ƒê√°nh d·∫•u file ƒë√£ n·∫°p
                    self._mark_document_as_ingested(file_path)
                    print(f"‚úÖ ƒê√£ ƒë·ªçc: {filename}")
                    
                except Exception as e:
                    print(f"‚ùå L·ªói file {filename}: {e}")
        
        if skipped_files > 0:
            print(f"\nüìä T√≥ml·∫°i: B·ªè qua {skipped_files} file ƒë√£ n·∫°p tr∆∞·ªõc ƒë√≥")

        if not documents:
            if skipped_files > 0:
                print("üí° T·∫•t c·∫£ file ƒë√£ ƒë∆∞·ª£c n·∫°p r·ªìi. Kh√¥ng c√≥ g√¨ m·ªõi ƒë·ªÉ x·ª≠ l√Ω.")
            else:
                print("‚ùå Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o.")
            return

        # 2. Chunking khoa h·ªçc (s·ª≠ d·ª•ng ChunkingService)
        print(f"\n‚úÇÔ∏è  ƒêang c·∫Øt nh·ªè vƒÉn b·∫£n...")
        splits = self.chunking_service.split_documents(documents)
        total_chunks = len(splits)
        
        print(f"\nüì¶ T·ªïng c·ªông: {total_chunks} ƒëo·∫°n vƒÉn b·∫£n.")
        print(f"‚öôÔ∏è  C·∫•u h√¨nh Chunking:")
        print(f"   - chunk_size: {self.chunking_service.chunk_size} k√Ω t·ª±")
        print(f"   - chunk_overlap: {self.chunking_service.chunk_overlap} k√Ω t·ª± ({self.chunking_service.chunk_overlap_percent*100:.0f}%) - T·ªêI ∆ØU")
        print(f"   - separators: {self.chunking_service.separators}")
        print(f"   - M·ª•c ƒë√≠ch: Gi·ªØ ng·ªØ c·∫£nh ƒêi·ªÅu/Kho·∫£n kh√¥ng b·ªã c·∫Øt qu√£ng")

        # 3. N·∫°p v√†o Vector DB theo ch·∫ø ƒë·ªô "An To√†n Tuy·ªát ƒê·ªëi"
        batch_size = 1 
        print(f"üöÄ ƒêang n·∫°p t·ª´ng b∆∞·ªõc (C·ª±c ch·∫≠m) ƒë·ªÉ tr√°nh b·ªã ch·∫∑n...")
        
        for i in range(0, total_chunks, batch_size):
            batch = splits[i:i + batch_size]
            try:
                self.vector_store.add_documents(documents=batch)
                print(f"   ‚û§ ƒê√£ n·∫°p th√†nh c√¥ng: {i + 1}/{total_chunks}")
                time.sleep(10)  # Ngh·ªâ 10 gi√¢y m·ªói ƒëo·∫°n
            except Exception as e:
                print(f"‚ö†Ô∏è ƒêang ƒë·ª£i 60 gi√¢y do Google qu√° t·∫£i: {e}")
                time.sleep(60)
                self.vector_store.add_documents(documents=batch)

    def ask_question(self, question: str) -> str:
        """Nhi·ªám v·ª• Tu·∫ßn 4: Truy v·∫•n th√¥ng minh"""
        retriever = self.vector_store.as_retriever(search_kwargs={"k": 5})
        
        system_prompt = (
            "B·∫°n l√† tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ sinh vi√™n d·ª±a tr√™n t√†i li·ªáu n·ªôi b·ªô c·ªßa nh√† tr∆∞·ªùng. "
            "Ch·ªâ s·ª≠ d·ª•ng c√°c ƒëo·∫°n vƒÉn b·∫£n d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. "
            "N·∫øu th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu, h√£y n√≥i 'T√¥i kh√¥ng bi·∫øt'. "
            "C√¢u tr·∫£ l·ªùi c·∫ßn ng·∫Øn g·ªçn, ch√≠nh x√°c v√† l·ªãch s·ª±."
            "\n\n"
            "{context}"
        )

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"),
        ])

        question_answer_chain = create_stuff_documents_chain(self.llm, prompt)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        response = rag_chain.invoke({"input": question})
        return response["answer"]

rag_service = RAGService()